{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer Machine Translation Inference\n",
    "This notebook demonstrates how to use the trained transformer model for English to Italian translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from model import build_transformer\n",
    "from config import get_config, latest_weights_file_path\n",
    "from tokenizers import Tokenizer\n",
    "from train_wb import greedy_decode\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Load configuration\n",
    "config = get_config()\n",
    "\n",
    "# Device selection\n",
    "device = torch.device('cuda' if torch.cuda.is_available() \n",
    "                     else 'mps' if torch.backends.mps.is_available() \n",
    "                     else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error loading model weights: No model weights found in the specified location\n",
      "Please ensure the weights file exists and the path in config is correct\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "No model weights found in the specified location",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 18\u001b[0m\n\u001b[1;32m     16\u001b[0m model_path \u001b[38;5;241m=\u001b[39m latest_weights_file_path(config)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_path:\n\u001b[0;32m---> 18\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo model weights found in the specified location\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# Load the model state\u001b[39;00m\n\u001b[1;32m     21\u001b[0m state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(model_path, map_location\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: No model weights found in the specified location"
     ]
    }
   ],
   "source": [
    "# Load tokenizers\n",
    "tokenizer_src = Tokenizer.from_file(config['tokenizer_file'].format(config['lang_src']))\n",
    "tokenizer_tgt = Tokenizer.from_file(config['tokenizer_file'].format(config['lang_tgt']))\n",
    "\n",
    "# Load model\n",
    "model = build_transformer(\n",
    "    tokenizer_src.get_vocab_size(),\n",
    "    tokenizer_tgt.get_vocab_size(),\n",
    "    config['seq_len'],\n",
    "    config['seq_len'],\n",
    "    d_model=config['d_model']\n",
    ").to(device)\n",
    "\n",
<<<<<<< HEAD
    "# Load latest weights\n",
    "                     # Load model\n",
    "model = build_transformer(\n",
    "    tokenizer_src.get_vocab_size(),\n",
    "    tokenizer_tgt.get_vocab_size(),\n",
    "    config['seq_len'],\n",
    "    config['seq_len'],\n",
    "    d_model=config['d_model']\n",
    ").to(device)\n",
    "\n",
    "# Try to find the latest weights file\n",
    "try:\n",
    "    model_path = latest_weights_file_path(config)\n",
    "    if model_path is None:\n",
    "        raise FileNotFoundError(\"No trained model weights found!\")\n",
    "        \n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    print(f\"Loaded weights from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading weights: {str(e)}\")\n",
    "    print(\"Proceeding with untrained model\")\n",
    "if model_path is None:\n",
    "    raise FileNotFoundError(\"No trained model weights found!\")\n",
=======
    "# Load weights with error handling\n",
    "try:\n",
    "    model_path = latest_weights_file_path(config)\n",
    "    if not model_path:\n",
    "        raise FileNotFoundError(\"No model weights found in the specified location\")\n",
>>>>>>> ef36e20 (edits)
    "    \n",
    "    # Load the model state\n",
    "    state = torch.load(model_path, map_location=device)\n",
    "    model.load_state_dict(state['model_state_dict'])\n",
    "    print(f\"Loaded weights from {model_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading model weights: {str(e)}\")\n",
    "    print(\"Please ensure the weights file exists and the path in config is correct\")\n",
    "    # You might want to provide a fallback path or raise the exception\n",
    "    # model_path = \"your_fallback_path/weights.pt\"  # Uncomment to use a fallback path\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence: str):\n",
    "    \"\"\"Translate an English sentence to Italian\"\"\"\n",
    "    # Tokenize the source text\n",
    "    encoder_input = tokenizer_src.encode(sentence)\n",
    "    encoder_input_ids = torch.tensor([encoder_input.ids]).to(device)\n",
    "    \n",
    "    # Create source mask\n",
    "    encoder_mask = torch.ones(1, 1, len(encoder_input.ids)).to(device)\n",
    "    \n",
    "    # Translate\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        translated_tokens = greedy_decode(\n",
    "            model, \n",
    "            encoder_input_ids, \n",
    "            encoder_mask,\n",
    "            tokenizer_src,\n",
    "            tokenizer_tgt,\n",
    "            config['seq_len'],\n",
    "            device\n",
    "        )\n",
    "    \n",
    "    # Decode the translated tokens\n",
    "    translated_text = tokenizer_tgt.decode(translated_tokens.detach().cpu().numpy())\n",
    "    \n",
    "    # Clean up special tokens\n",
    "    translated_text = translated_text.replace('[SOS]', '').replace('[EOS]', '').strip()\n",
    "    \n",
    "    return translated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the translation\n",
    "test_sentences = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"I love reading books.\",\n",
    "    \"The weather is beautiful today.\",\n",
    "    \"Can you help me find my way to the train station?\"\n",
    "]\n",
    "\n",
    "print(\"English to Italian Translation Examples:\")\n",
    "print(\"-\" * 50)\n",
    "for sentence in test_sentences:\n",
    "    translation = translate(sentence)\n",
    "    print(f\"English: {sentence}\")\n",
    "    print(f\"Italian: {translation}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive translation\n",
    "while True:\n",
    "    text = input(\"Enter English text to translate (or 'q' to quit): \")\n",
    "    if text.lower() == 'q':\n",
    "        break\n",
    "    \n",
    "    translation = translate(text)\n",
    "    print(f\"Italian translation: {translation}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_central_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
