<<<<<<< HEAD
![alt text](image.png)

some observations : 
- 'mps' is very poor for training transformers 
- i got 3% of one epoch when i was training on mps overnight but when i trained it on NVIDA L4 GPU it only took 3 sec to go to that 3% lol

=======
# something i see that when i trained on a 4x cpu on lightning ai it worked soooo well but when i trained it on a single mq gpu using mps backend it didnt worked well there was not even a single epoch in 8 hours(lol the time i slept)
>>>>>>> origin/main
